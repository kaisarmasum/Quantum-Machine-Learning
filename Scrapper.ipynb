{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHrQqNeZS9w7Rpym9xe6nv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaisarmasum/Quantum-Machine-Learning/blob/main/Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaJjdlniJJJ5",
        "outputId": "b12cd964-37af-4f50-b50d-79cec67d2e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzYRBW1oKfoR",
        "outputId": "2ee3fb8f-05f9-40cf-bb25-f5eb9083cbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# List of IMDb movie URLs to scrape (replace with your own URLs)\n",
        "movie_urls = [\n",
        "    \"https://www.imdb.com/title/tt0111161/reviews\",  # The Shawshank Redemption\n",
        "    \"https://www.imdb.com/title/tt0068646/reviews\",  # The Godfather\n",
        "    \"https://www.imdb.com/title/tt0110912/reviews\",  # Pulp Fiction\n",
        "    \"https://www.imdb.com/title/tt0468569/reviews\",  # The Dark Knight\n",
        "    \"https://www.imdb.com/title/tt0137523/reviews\",  # Fight Club\n",
        "    \"https://www.imdb.com/title/tt0109830/reviews\",  # Forrest Gump\n",
        "    \"https://www.imdb.com/title/tt1375666/reviews\",  # Inception\n",
        "    \"https://www.imdb.com/title/tt0071562/reviews\",  # The Godfather: Part II\n",
        "    \"https://www.imdb.com/title/tt0167260/reviews\",  # The Lord of the Rings: The Return of the King\n",
        "    \"https://www.imdb.com/title/tt0120737/reviews\",  # The Lord of the Rings: The Fellowship of the Ring\n",
        "    \"https://www.imdb.com/title/tt0241527/reviews\",  # Spirited Away\n",
        "    \"https://www.imdb.com/title/tt0060196/reviews\",  # The Good, the Bad and the Ugly\n",
        "    \"https://www.imdb.com/title/tt0108052/reviews\",  # Schindler's List\n",
        "    \"https://www.imdb.com/title/tt0114814/reviews\",  # The Usual Suspects\n",
        "    \"https://www.imdb.com/title/tt0133093/reviews\",  # The Matrix\n",
        "    \"https://www.imdb.com/title/tt0073486/reviews\",  # One Flew Over the Cuckoo's Nest\n",
        "    \"https://www.imdb.com/title/tt0407887/reviews\",  # The Departed\n",
        "    \"https://www.imdb.com/title/tt0088763/reviews\",  # Back to the Future\n",
        "    \"https://www.imdb.com/title/tt2582802/reviews\",  # Whiplash\n",
        "    \"https://www.imdb.com/title/tt1853728/reviews\",  # Django Unchained\n",
        "    \"https://www.imdb.com/title/tt0038650/reviews\",  # It's a Wonderful Life\n",
        "    \"https://www.imdb.com/title/tt0082971/reviews\",  # Raiders of the Lost Ark\n",
        "    \"https://www.imdb.com/title/tt0405094/reviews\",  # The Lives of Others\n",
        "    \"https://www.imdb.com/title/tt0078788/reviews\",  # Apocalypse Now\n",
        "]\n",
        "\n",
        "# Initialize empty lists to store review data\n",
        "movie_titles = []\n",
        "review_texts = []\n",
        "ratings = []\n",
        "\n",
        "# Set the number of pages to scrape for each movie (adjust as needed)\n",
        "num_pages_to_scrape = 20\n",
        "\n",
        "# Function to scrape reviews for a movie\n",
        "def scrape_reviews(movie_url):\n",
        "    response = requests.get(movie_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_containers = soup.find_all('div', class_='text show-more__control')\n",
        "        rating_containers = soup.find_all('span', class_='rating-other-user-rating')\n",
        "\n",
        "        for review_container, rating_container in zip(review_containers, rating_containers):\n",
        "            review_text = review_container.get_text(strip=True)\n",
        "            rating_text = rating_container.text.strip().split('/')[0].strip()\n",
        "\n",
        "            movie_titles.append(movie_url.split('/')[4])  # Extract movie title from URL\n",
        "            review_texts.append(review_text)\n",
        "            ratings.append(rating_text)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page {movie_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# Iterate through multiple movies\n",
        "for movie_url in movie_urls:\n",
        "    # Iterate through multiple pages and scrape reviews for each movie\n",
        "    for page_num in range(1, num_pages_to_scrape + 1):\n",
        "        page_url = f\"{movie_url}?page={page_num}\"\n",
        "        scrape_reviews(page_url)\n",
        "\n",
        "# Create a DataFrame from the scraped data\n",
        "data = {'Movie Title': movie_titles, 'Review Text': review_texts, 'Rating': ratings}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('imdb_reviews.csv', index=False)\n",
        "\n",
        "print(f\"Scraped {len(df)} reviews and saved to 'imdb_reviews.csv'.\")"
      ],
      "metadata": {
        "id": "NCBfWIabNGqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3883b2e7-89be-4733-9a77-80c49480e046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 10560 reviews and saved to 'imdb_reviews.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kT_qRmz5e4AO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}