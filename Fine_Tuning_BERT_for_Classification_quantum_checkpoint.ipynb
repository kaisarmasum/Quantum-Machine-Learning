{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaisarmasum/Quantum-Machine-Learning/blob/main/Fine_Tuning_BERT_for_Classification_quantum_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fpWbwsrP3pH"
      },
      "source": [
        "# Fine-tuning BERT for Text Classification through Hybrid Transfer Learning for Natural Language Processing (NLP)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "rYQbiZDrZ9Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "id": "-rgbs8xp_UwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34XRaMevP3pJ"
      },
      "source": [
        "This is an example of a hybrid network constructed using classical-to-quantum transfer learning to fine-tune BERT for text categorization.\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers is what BERT stands for. It is intended to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context. Therefore, the pre-trained BERT model may be fine-tuned with just one extra output layer to generate state-of-the-art models for a variety of NLP applications.\"\n",
        "BERT is pre-trained on a massive corpus of unlabeled text, including the whole of Wikipedia (that's 2,500 million words!) and Book Corpus (800 million words).\n",
        "\n",
        "Now, we will fine-tune a BERT model to classify text using a quantum circuit that has been dressed.\n",
        "We own an assortment of SMS texts. A portion of these SMS are spam, but the remainder are legitimate. Our mission is to develop a system that can automatically determine whether or not a message is spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qRpLspZzcPu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9XpTVSP3pK"
      },
      "source": [
        "Configure PyTorch to use CUDA, only if available. Otherwise simply use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_27_SN86z55S"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\") #\"cuda:0\" if torch.cuda.is_available() else\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOamAmY1P3pK"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "There are two columns in the dataset: label and text. The column \"text\" holds the message content, whereas \"label\" is a binary variable where 1 indicates that the message is spam and 0 indicates that it is not spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wydZ9Zal0BO0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"spamdata.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5skbi_Tt0Hxf"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4C04ert0Z8e"
      },
      "outputs": [],
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duyf9CrhP3pL"
      },
      "source": [
        "This dataset will now be divided into three sets: train, validation, and test.\n",
        "\n",
        "We will fine-tune the model using the train set and the validation set, and make predictions for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxQot9h0cxN"
      },
      "outputs": [],
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'],\n",
        "                                                                    random_state=2018,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLRo2fyvP3pL"
      },
      "source": [
        "## Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hup7IdNX0g0B"
      },
      "outputs": [],
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCjYpLZTP3pL"
      },
      "source": [
        "## Tokenize the Sentences\n",
        "\n",
        "Since the messages (text) in the dataset are of variable lengths, we will employ padding to ensure that the length of each message is the same. We can pad messages using the maximum sequence length. To determine the correct padding length, we may also examine the distribution of sequence lengths in the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppBzSzrs0mJT"
      },
      "outputs": [],
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt_DKtdWP3pM"
      },
      "source": [
        "It is evident that the majority of the texts include little more than 25 words. In contrast to the maximum length of 175, If we choose 175 as the padding length, then all input sequences will have a length of 175 and the majority of tokens in those sequences will be padding tokens, which will not help the model learn anything useful and will also slow down training.\n",
        "\n",
        "We will thus set the padding length to 25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paFDRrNn0vLw"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkRN2VKX00hc"
      },
      "outputs": [],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Z-UYhNP3pM"
      },
      "source": [
        "## convert the integer sequences to tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLsSDtyx05JA"
      },
      "outputs": [],
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6M6kTmP3pM"
      },
      "source": [
        "## Create DataLoaders\n",
        "\n",
        "Now, dataloaders will be created for both the train and validation sets. During the training phase, these dataloaders will send batches of train data and validation data to the model as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbFg4BYa0-1C"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786BqT_GP3pM"
      },
      "source": [
        "## Freeze BERT Parameters\n",
        "\n",
        "We freeze all the layers of the BERT model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4ssjHXN1GS4"
      },
      "outputs": [],
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFB4diffP3pM"
      },
      "source": [
        "## Define Model Architecture Hybrid transfer learning model (classical-to-quantum).\n",
        "\n",
        "Setting of the main parameters of the network model and of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSX8NZ_-1Umt"
      },
      "outputs": [],
      "source": [
        "n_qubits = 2                     # Number of qubits\n",
        "q_depth = 4                      # Depth of the quantum circuit (number of variational layers)\n",
        "max_layers = 15                  # Keep 15 even if not all are used.\n",
        "q_delta = 0.01                   # Initial spread of random quantum weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBHNwijfP3pN"
      },
      "source": [
        "Let us initialize a PennyLane device with the default simulator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbI_r_8L3es7"
      },
      "outputs": [],
      "source": [
        "dev = qml.device('default.qubit', wires=n_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHWv1_y1P3pN"
      },
      "source": [
        "First, we define the quantum layers that constitute the quantum circuit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RlbOvsY3ho4"
      },
      "outputs": [],
      "source": [
        "def H_layer(nqubits):\n",
        "    \"\"\"Layer of single-qubit Hadamard gates.\n",
        "    \"\"\"\n",
        "    for idx in range(nqubits):\n",
        "        qml.Hadamard(wires=idx)\n",
        "\n",
        "def RY_layer(w):\n",
        "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
        "    \"\"\"\n",
        "    for idx, element in enumerate(w):\n",
        "        qml.RY(element, wires=idx)\n",
        "\n",
        "def entangling_layer(nqubits):\n",
        "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
        "    \"\"\"\n",
        "    # In other words it should apply something like :\n",
        "    #CNOT  CNOT  CNOT  CNOT...  CNOT\n",
        "    #   CNOT  CNOT  CNOT...  CNOT\n",
        "    for i in range(0, nqubits - 1, 2): #loop over even indices: i=0,2,...N-2\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "    for i in range(1, nqubits - 1, 2): #loop over odd indices:  i=1,3,...N-3\n",
        "        qml.CNOT(wires=[i, i + 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neROwE5MP3pN"
      },
      "source": [
        "Use the PennyLane `qnode` decorator to define the quantum circuit. The circuit has the conventional structure of a variational quantum circuit:\n",
        "\n",
        "1. All qubits are started in a balanced superposition of up and down states, then rotated based on the input parameters (local embedding);\n",
        "2. A series of trainable rotation layers and continual entangling layers are applied in succession. This block does the majority of the computations required to solve the categorization issue.\n",
        "3. Eventually, the local expectation value of the Z operator is measured for each qubit. This generates a conventional output vector suitable for further post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxqI7S0f3v3X"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface='torch')\n",
        "def q_net(q_in, q_weights_flat):\n",
        "\n",
        "        # Reshape weights\n",
        "        q_weights = q_weights_flat.reshape(max_layers, n_qubits)\n",
        "\n",
        "        # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
        "        H_layer(n_qubits)\n",
        "\n",
        "        # Embed features in the quantum node\n",
        "        RY_layer(q_in)\n",
        "\n",
        "        # Sequence of trainable variational layers\n",
        "        for k in range(q_depth):\n",
        "            entangling_layer(n_qubits)\n",
        "            RY_layer(q_weights[k + 1])\n",
        "\n",
        "        # Expectation values in the Z basis\n",
        "        return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIYA5yQGP3pN"
      },
      "source": [
        "Now, we can define a customized `torch.nn.Module` depicting a dressed quantum circuit.\n",
        "This string is composed of:\n",
        "\n",
        "1. A classical pre-processing layer (`nn.Linear`)\n",
        "2. A classical activation function (`F.tanh`)\n",
        "3. A constant `np.pi/2.0` scaling factor.\n",
        "4. The quantum circuit described before (`q net`)\n",
        "5. A classical post-processing layer (`nn.Linear`)\n",
        "\n",
        "Input to the module is a collection of vectors with 768real parameters (features), and output is a collection of vectors with two real outputs (associated with the two classes of messages: spam and notspam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKmy6bPV4auJ"
      },
      "outputs": [],
      "source": [
        "class Quantumnet(nn.Module):\n",
        "\n",
        "  def __init__(self, bert):\n",
        "\n",
        "      super(Quantumnet, self).__init__()\n",
        "\n",
        "      self.bert = bert\n",
        "\n",
        "      self.pre_net = nn.Linear(768, n_qubits)\n",
        "      self.q_params = nn.Parameter(q_delta * torch.randn(max_layers * n_qubits))\n",
        "      self.post_net = nn.Linear(n_qubits, 2)\n",
        "\n",
        "  def forward(self, sent_id, mask):\n",
        "\n",
        "    _, cls_hs = self.bert(sent_id, attention_mask=mask,  return_dict=False)\n",
        "\n",
        "    pre_out = self.pre_net(cls_hs)\n",
        "    q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
        "\n",
        "    # Apply the quantum circuit to each element of the batch and append to q_out\n",
        "    q_out = torch.Tensor(0, n_qubits)\n",
        "    q_out = q_out.to(device)\n",
        "    for elem in q_in:\n",
        "      q_out_elem = q_net(elem,self.q_params).float().unsqueeze(0)\n",
        "      q_out = torch.cat((q_out, q_out_elem))\n",
        "\n",
        "\n",
        "    return self.post_net(q_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyDKJdWp-oaN"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = Quantumnet(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_sRTnRkP3pN"
      },
      "source": [
        "We will use AdamW as our optimizer. It is an improved version of the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkuBoAJ5BpGz"
      },
      "outputs": [],
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlr_zHzTP3pO"
      },
      "source": [
        "## Find Class Weights\n",
        "\n",
        "Our dataset has an imbalance between classes. The vast majority of messages are not spam. Therefore, we will first calculate class weights for the labels in the train set and then send these weights to the loss function so that the class imbalance is taken care of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8H0FfTkBw1H"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight = \"balanced\",\n",
        "                                 classes = np.unique(train_labels),\n",
        "                                 y = train_labels)\n",
        "\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Et9JHRKB0Dg"
      },
      "outputs": [],
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDgm_MQlP3pO"
      },
      "source": [
        "## Fine-Tune BERT\n",
        "\n",
        "So far, we have described the model's architecture, specified the optimizer and loss function, and prepared the dataloaders. Now we must  define a couple of functions to train (fine-tune) and evaluate the model, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX8M9Vj_B3Yu"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyyqwKzhID7R"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "  print(\"\\nEvaluating...\")\n",
        "\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWRkXJYtP3pP"
      },
      "source": [
        "## Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO_mLlxkIHtQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9nn3yCKP3pP"
      },
      "source": [
        "## Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grrKnPPVITJY"
      },
      "outputs": [],
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NyL3q_nP3pQ"
      },
      "source": [
        "## Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgnPWsAzPP_0"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raobbm1ieR0j"
      },
      "outputs": [],
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4DBSCteewjj"
      },
      "outputs": [],
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "xN4auyD9WxSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}